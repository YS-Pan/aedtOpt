# batch_eval.py
from __future__ import annotations

import json
import os
import re
import shutil
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Sequence

from optConfig import NUMBER_OF_OBJECTIVES

SCRIPT_DIR = Path(__file__).resolve().parent
JOBS_DIR = SCRIPT_DIR / "jobs"
TEMPLATE_DIR = SCRIPT_DIR / "job_template"
JOB_ENVIRONMENT = os.environ.get(
    "CONDOR_ENVIRONMENT",
    "USERPROFILE=.\\_home;HOME=.\\_home;APPDATA=.\\_appdata;LOCALAPPDATA=.\\_localappdata;TEMP=.\\_tmp;TMP=.\\_tmp",
)
SUBMIT_EXE = os.environ.get("CONDOR_SUBMIT_EXE", "condor_submit")
TIMEOUT_SEC = float(os.environ.get("CONDOR_TIMEOUT_SEC", "18000"))
POLL_SEC = float(os.environ.get("CONDOR_POLL_SEC", "30"))

REQUEST_CPUS = int(os.environ.get("CONDOR_REQUEST_CPUS", "4"))
REQUEST_MEMORY = os.environ.get("CONDOR_REQUEST_MEMORY", "8GB")
REQUEST_DISK = os.environ.get("CONDOR_REQUEST_DISK", "2GB")

LOAD_PROFILE = os.environ.get("CONDOR_LOAD_PROFILE", "True").lower() in {"1", "true", "yes"}

DEFAULT_COST = 1.0

_CLUSTER_RE = re.compile(r"submitted to cluster\s+(\d+)", re.IGNORECASE)

_SKIP_INPUTS = {
    "__pycache__",
    "job.sub",
    "cost.json",
    "stdout.txt",
    "stderr.txt",
    "condor.log",
    "condor_submit.stdout.txt",
    "condor_submit.stderr.txt",
    "cluster.id",
}

_TERMINAL_LOG_MARKERS = ("Job terminated", "Job was held", "Job was aborted", "Job was removed")


def _ts() -> str:
    dt = datetime.now()
    return dt.strftime("%y-%m-%d-%H-%M-%S-") + f"{dt.microsecond // 1000:03d}"


def new_job_folder(jobs_root: Path = JOBS_DIR) -> Path:
    jobs_root.mkdir(parents=True, exist_ok=True)
    for k in range(1000):
        name = f"job_{_ts()}" + ("" if k == 0 else f"_{k:03d}")
        p = jobs_root / name
        try:
            p.mkdir()
            return p
        except FileExistsError:
            time.sleep(0.001)
    raise RuntimeError("Failed to create a unique job folder name.")


def copy_files(job_dir: Path, template_dir: Path = TEMPLATE_DIR) -> None:
    if not template_dir.is_dir():
        raise FileNotFoundError(f"job_template not found: {template_dir}")

    for src in template_dir.iterdir():
        if src.name == "__pycache__":
            continue
        dst = job_dir / src.name
        if src.is_dir():
            shutil.copytree(src, dst)
        else:
            shutil.copy2(src, dst)

    # Requirement: copy parameters_constraints_class.py into the job folder
    src_pc = SCRIPT_DIR / "parameters_constraints_class.py"
    if src_pc.is_file():
        shutil.copy2(src_pc, job_dir / "parameters_constraints_class.py")


def clone_and_modify_parameters_file(ind: Sequence[float], job_dir: Path) -> None:
    from parameters_constraints import CONSTRAINTS, PARAMETERS
    from parameters_constraints_class import para

    if len(ind) != len(PARAMETERS):
        raise ValueError(f"Individual length {len(ind)} != NDIM {len(PARAMETERS)}")

    def fmt(x: float) -> str:
        return f"{float(x):.16g}"

    lines: List[str] = ["from parameters_constraints_class import para", "", "PARAMETERS = ("]
    for base_p, x in zip(PARAMETERS, ind):
        p = para(str(base_p.name), tuple(base_p.ranges), normValue=float(x), value=float("nan"), unit=str(base_p.unit))
        p.denorm(update=True)
        lines.append(
            f"    para({p.name!r}, {p.ranges!r}, value={fmt(p.value)}, normValue={fmt(p.normValue)}, unit={p.unit!r}),"
        )
    lines += [")", "", f"CONSTRAINTS = {tuple(CONSTRAINTS)!r}", ""]

    (job_dir / "parameters_constraints.py").write_text("\n".join(lines), encoding="utf-8", newline="\n")


def _q(s: str) -> str:
    # Quote only when needed for submit-file comma-separated lists
    return f'"{s}"' if any(c in s for c in (" ", ",")) else s


def generate_jobSub(job_dir: Path) -> Path:
    exe = "workflow.py"  # run like your working test submit file

    # transfer_input_files: everything in the folder except condor artifacts and the executable itself
    inputs = [
        _q(p.name)
        for p in sorted(job_dir.iterdir(), key=lambda x: x.name.lower())
        if p.name not in _SKIP_INPUTS and p.name != exe
    ]

    lines: List[str] = [
        "# Auto-generated by batch_eval.py",
        "universe = vanilla",
        "",
        f"executable = {exe}",
        "initialdir = .",
        "getenv     = False",
    ]
    if JOB_ENVIRONMENT:
        lines.append(f'environment = "{JOB_ENVIRONMENT}"')
    if LOAD_PROFILE:
        lines.append("load_profile = True")

    lines += [
        "",
        "should_transfer_files   = YES",
        "when_to_transfer_output = ON_EXIT",
        "transfer_executable     = True",
        f"transfer_input_files    = {','.join(inputs)}",
        "",
        "output = stdout.txt",
        "error  = stderr.txt",
        "log    = condor.log",
        "",
        f"request_cpus   = {REQUEST_CPUS}",
        f"request_memory = {REQUEST_MEMORY}",
        f"request_disk   = {REQUEST_DISK}",
        "",
        "notification = never",
        "transfer_output_files = cost.json,rawData,batch.log,chokeST_V2_1_Half.aedt",
        "queue 1",
        "",
    ]

    submit_text = "\n".join(lines)
    if not submit_text.endswith("\n"):
        submit_text += "\n"

    path = job_dir / "job.sub"
    path.write_text(submit_text, encoding="utf-8", newline="\n")
    return path


def submit(job_dir: Path) -> Optional[int]:
    cp = subprocess.run(
        [SUBMIT_EXE, "job.sub"],
        cwd=str(job_dir),
        capture_output=True,
        text=True,
        check=False,
    )
    (job_dir / "condor_submit.stdout.txt").write_text(cp.stdout or "", encoding="utf-8", newline="\n")
    (job_dir / "condor_submit.stderr.txt").write_text(cp.stderr or "", encoding="utf-8", newline="\n")
    if cp.returncode != 0:
        raise RuntimeError(f"condor_submit failed in {job_dir}\n{cp.stdout}\n{cp.stderr}")

    m = _CLUSTER_RE.search(cp.stdout or "")
    cid = int(m.group(1)) if m else None
    if cid is not None:
        (job_dir / "cluster.id").write_text(str(cid), encoding="utf-8", newline="\n")
    return cid


def _try_load_cost_vector(cost_path: Path) -> Optional[List[float]]:
    """
    Load cost vector from cost.json.
    Expects a "costs" array with exactly NUMBER_OF_OBJECTIVES elements.
    Does not read or validate named cost keys - those are for human readability only.
    """
    if not cost_path.exists():
        return None
    try:
        data = json.loads(cost_path.read_text(encoding="utf-8"))
    except Exception:
        return None

    nobj = int(NUMBER_OF_OBJECTIVES)

    # Read the ordered "costs" array (machine-readable format)
    if isinstance(data, dict) and "costs" in data:
        costs = data["costs"]
        if isinstance(costs, list) and len(costs) == nobj:
            return [float(x) for x in costs]

    # Fallback: if data is a list directly
    if isinstance(data, list) and len(data) == nobj:
        return [float(x) for x in data]

    return None


def _write_error_cost(cost_path: Path) -> None:
    """Write a default error cost.json with all costs set to DEFAULT_COST."""
    nobj = int(NUMBER_OF_OBJECTIVES)
    cost_path.write_text(
        json.dumps({"costs": [DEFAULT_COST] * nobj}, indent=2),
        encoding="utf-8",
        newline="\n",
    )


def _is_terminal(job_dir: Path) -> bool:
    if _try_load_cost_vector(job_dir / "cost.json") is not None:
        return True
    logp = job_dir / "condor.log"
    if logp.exists():
        txt = logp.read_text(encoding="utf-8", errors="ignore")
        if any(m in txt for m in _TERMINAL_LOG_MARKERS):
            if _try_load_cost_vector(job_dir / "cost.json") is None:
                _write_error_cost(job_dir / "cost.json")
            return True
    return False


def batch_evaluate(normedPop: Sequence[Sequence[float]]) -> List[List[float]]:
    nobj = int(NUMBER_OF_OBJECTIVES)
    job_dirs: List[Path] = []

    for i, ind in enumerate(normedPop):
        d = new_job_folder()
        copy_files(d)

        # Ensure rawData exists so transfer_output_files=rawData won't hold the job
        (d / "rawData").mkdir(parents=True, exist_ok=True)

        clone_and_modify_parameters_file(ind, d)
        generate_jobSub(d)
        cid = submit(d)
        job_dirs.append(d)
        print(f"[submit] {i+1}/{len(normedPop)} -> {d.name} (cluster={cid})", flush=True)

    pending = set(job_dirs)
    deadline = time.monotonic() + TIMEOUT_SEC
    while pending and time.monotonic() < deadline:
        for d in list(pending):
            if _is_terminal(d):
                pending.remove(d)
        print(f"[wait] finished {len(job_dirs) - len(pending)}/{len(job_dirs)}", flush=True)
        if pending:
            time.sleep(POLL_SEC)

    # timeout => mark remaining as error
    for d in pending:
        _write_error_cost(d / "cost.json")

    out: List[List[float]] = []
    for d in job_dirs:
        v = _try_load_cost_vector(d / "cost.json")
        out.append(v if v is not None else [DEFAULT_COST] * nobj)
    return out